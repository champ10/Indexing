{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# IR Assignment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import os, sys\n",
    "import numpy as np\n",
    "import codecs\n",
    "import nltk\n",
    "from collections import defaultdict, OrderedDict\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import pickle\n",
    "import logging\n",
    "from datetime import datetime\n",
    "import time\n",
    "from IR import * \n",
    "index_file = 'index.pkl'\n",
    "encoding = 'latin1'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pre-processor Class\n",
    "Text preprocessing class. Responsible for term normalization using stop word and lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Normaliser(object):\n",
    "    def __init__(self, lemmatizer=None, stop_words=None, dictionary=None, lower_case=True):\n",
    "        self.lemmatizer = lemmatizer\n",
    "        self.stop_words = stop_words\n",
    "        print(\"The number of stop words is %d\" % (len(self.stop_words)))\n",
    "        print(\"lemmatizer is \" + str(self.lemmatizer))\n",
    "        self.dictionary = dictionary\n",
    "        self.lower_case = lower_case\n",
    "\n",
    "    def normalise(self, token):\n",
    "        '''\n",
    "        normalization  using stop word removal and lemmatization\n",
    "        :param token:\n",
    "        :return:\n",
    "        '''\n",
    "        if self.lower_case:\n",
    "            token = token.lower()\n",
    "        if token in self.stop_words:\n",
    "            return None\n",
    "        if self.lemmatizer:\n",
    "            token = self.lemmatizer.lemmatize(token)\n",
    "        if self.dictionary:\n",
    "            if token not in self.dictionary:\n",
    "                return None\n",
    "        return token\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Indexer Class\n",
    "Indexer class responsible for generating ,storing inverted indexes and searching queries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Indexer(object):\n",
    "    def __init__(self, tokeniser, normaliser=None):\n",
    "        self.tokeniser = tokeniser\n",
    "        self.normaliser = normaliser\n",
    "        self.inverted_index = defaultdict(PostingList)\n",
    "        # total number of documents\n",
    "        self.N = 0\n",
    "        self.document_lengths = defaultdict(float)\n",
    "        self.dl = defaultdict(int)\n",
    "        self.dic = defaultdict(int)\n",
    "\n",
    "    def index(self, docID, text):\n",
    "        '''\n",
    "        Generate index from document\n",
    "\n",
    "        :param docID:\n",
    "        :param text:\n",
    "        :return:\n",
    "        '''\n",
    "        tokens = self.tokeniser.tokenize(text)\n",
    "        token_position = 0\n",
    "        term_documents = defaultdict(TermDocument)\n",
    "        for token in tokens:\n",
    "            if self.normaliser:\n",
    "                token = self.normaliser.normalise(token)\n",
    "            if not token:\n",
    "                continue\n",
    "            term_document = term_documents[token]\n",
    "            term_document.tf += 1\n",
    "            term_document.positions.append(token_position)\n",
    "            token_position += 1\n",
    "            self.dl[docID] += 1\n",
    "            self.dic[token] += 1\n",
    "        # update the main index\n",
    "        for term, term_document in term_documents.items():\n",
    "            tf = term_document.tf\n",
    "            self.document_lengths[docID] += np.square(tf)\n",
    "            self.inverted_index[term].posts.append([docID, term_document])\n",
    "            self.inverted_index[term].df += 1\n",
    "        self.N += 1\n",
    "        self.document_lengths[docID] = np.sqrt(self.document_lengths[docID])\n",
    "        \n",
    "    def search(self, query):\n",
    "            '''\n",
    "            Searching query from indxes\n",
    "\n",
    "            :param query:\n",
    "            :return:\n",
    "            '''\n",
    "            results = defaultdict(float)\n",
    "            for term in query:\n",
    "                posting_list = self.inverted_index[term]\n",
    "                df = posting_list.df\n",
    "                idf = np.log(self.N / (df + 1))\n",
    "                posts = posting_list.posts\n",
    "                for post in posts:\n",
    "                    docID = post[0]\n",
    "                    term_document = post[1]\n",
    "                    tf = term_document.tf\n",
    "                    tfidf = tf * idf\n",
    "                    results[docID] += tfidf\n",
    "            for docID in results:\n",
    "                results[docID] = results[docID] / self.document_lengths[docID]\n",
    "            ranked_results = sorted(results.items(), key=lambda x: x[1], reverse=True)\n",
    "            return ranked_results\n",
    "        \n",
    "    def dump(self, filename):\n",
    "        '''\n",
    "        Storing raw indexes and pickel file\n",
    "\n",
    "        :param filename:\n",
    "        :return:\n",
    "        '''\n",
    "        logging.info(\"dumping index to %s\" % (filename))\n",
    "        with open(filename, 'wb') as outf:\n",
    "            pickle.dump((self.inverted_index, self.document_lengths, self.N, self.document_lengths, self.dl, self.dic),\n",
    "                        outf)\n",
    "        with open(filename + \".txt\", 'w') as outf:\n",
    "            for item in self.inverted_index.items():\n",
    "                term = item[0]\n",
    "                text = str(term)\n",
    "                for post in item[1].posts:\n",
    "                    doc_id = post[0]\n",
    "                    tf = post[1].tf\n",
    "                    text = text + \":\" + \"(doc:\" + str(doc_id) + \",tf:\" + str(tf) + \"), \"\n",
    "                # print(text)\n",
    "                outf.write(text)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Query Processor Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class QueryProcessor(object):\n",
    "    '''\n",
    "    Responsible for query preprocessing\n",
    "    '''\n",
    "\n",
    "    def __init__(self, tokeniser, normaliser=None):\n",
    "        self.tokeniser = tokeniser\n",
    "        self.normaliser = normaliser\n",
    "\n",
    "    def process(self, query):\n",
    "        tokens = self.tokeniser.tokenize(query)\n",
    "        query_terms = []\n",
    "        for token in tokens:\n",
    "            if self.normaliser:\n",
    "                token = self.normaliser.normalise(token)\n",
    "            if not token:\n",
    "                continue\n",
    "            query_terms.append(token)\n",
    "        query_terms = self._expandQuery(query_terms)\n",
    "        return query_terms\n",
    "    def _expandQuery(self, query_terms):\n",
    "        return query_terms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Code Usage\n",
    "Initialize pre-processer and indexer "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The number of stop words is 153\n",
      "lemmatizer is <WordNetLemmatizer>\n"
     ]
    }
   ],
   "source": [
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "normaliser = Normaliser(lemmatizer=lemmatizer, stop_words=nltk.corpus.stopwords.words('english'),\n",
    "                        dictionary=None,\n",
    "                        lower_case=True)\n",
    "tokeniser = NLTKWordTokenizer()\n",
    "indexer = Indexer(tokeniser, normaliser)\n",
    "query_processor = QueryProcessor(tokeniser, normaliser)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Index generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "05/04/2018 12:36:08 PM dumping index to Data/index.pkl\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Indexing 6 docs in Data/\n",
      "Done\n"
     ]
    }
   ],
   "source": [
    "base_dir = \"Data/\"\n",
    "docs = getfilenames(base_dir=base_dir)\n",
    "docs_length = len(docs)\n",
    "print(\"Indexing %d docs in %s\" % (docs_length, base_dir))\n",
    "docs_processed = 1\n",
    "for docID, filename in docs.items():\n",
    "    docs_processed += 1\n",
    "    filename = os.path.join(base_dir, filename)\n",
    "    text = getContent(filename, encoding=encoding)\n",
    "    indexer.index(docID, text)\n",
    "indexer.dump(base_dir + index_file)\n",
    "print(\"Done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3d:(doc:0,tf:3), \n",
      "\n",
      "radiology:(doc:0,tf:3), \n",
      "\n",
      "lab:(doc:0,tf:4), :(doc:3,tf:2), :(doc:5,tf:1), \n",
      "\n",
      "stanford:(doc:0,tf:13), :(doc:1,tf:19), :(doc:2,tf:2), :(doc:3,tf:1), :(doc:4,tf:5), :(doc:5,tf:9), \n",
      "\n",
      "university:(doc:0,tf:2), :(doc:1,tf:3), :(doc:3,tf:1), :(doc:4,tf:1), :(doc:5,tf:3), \n",
      "\n",
      "school:(doc:0,tf:6), :(doc:1,tf:8), :(doc:4,tf:4), :(doc:5,tf:1), \n",
      "\n",
      "medicine:(doc:0,tf:9), :(doc:1,tf:10), :(doc:4,tf:5), \n",
      "\n",
      "quantitative:(doc:0,tf:4), \n",
      "\n",
      "imaging:(doc:0,tf:5), \n",
      "\n",
      "department:(doc:0,tf:3), :(doc:1,tf:9), :(doc:3,tf:2), \n",
      "\n",
      "search:(doc:0,tf:1), :(doc:1,tf:1), :(doc:3,tf:1), :(doc:4,tf:1), :(doc:5,tf:2), \n",
      "\n",
      "site:(doc:0,tf:3), :(doc:1,tf:4), :(doc:4,tf:3), :(doc:5,tf:2), \n",
      "\n",
      "medical:(doc:0,tf:5), :(doc:1,tf:4), :(doc:4,tf:1), \n",
      "\n",
      "way:(doc:0,tf:3), :(doc:1,tf:3), :(doc:4,tf:2), \n",
      "\n",
      "give:(doc:0,tf:3), :(doc:1,tf:3), :(doc:4,tf:2), \n",
      "\n",
      "find:(doc:0,tf:5), :(doc:1,tf:5), :(doc:4,tf:2), \n",
      "\n",
      "person:(doc:0,tf:3), :(doc:1,tf:3), :(doc:2,tf:1), :(doc:4,tf:2), \n",
      "\n",
      "alumnus:(doc:0,tf:2), :(doc:1,tf:6), :(doc:4,tf:1), :(doc:5,tf:1), \n",
      "\n",
      "lane:(doc:0,tf:2), :(doc:1,tf:2), :(doc:4,tf:1), \n",
      "\n",
      "library:(doc:0,tf:3), :(doc:1,tf:3), :(doc:4,tf:1), :(doc:5,tf:3), \n",
      "\n",
      "u:(doc:0,tf:6), :(doc:1,tf:4), :(doc:3,tf:1), :(doc:4,tf:3), :(doc:5,tf:5), \n",
      "\n",
      "mission:(doc:0,tf:1), \n",
      "\n",
      "develop:(doc:0,tf:2), \n",
      "\n",
      "apply:(doc:0,tf:1), :(doc:2,tf:1), \n",
      "\n",
      "innovative:(doc:0,tf:1), :(doc:3,tf:1), \n",
      "\n",
      "technique:(doc:0,tf:2), \n",
      "\n",
      "efficient:(doc:0,tf:1), \n",
      "\n",
      "analysis:(doc:0,tf:3), \n",
      "\n",
      "display:(doc:0,tf:1), \n",
      "\n",
      "data:(doc:0,tf:2), \n",
      "\n",
      "interdisciplinary:(doc:0,tf:1), \n",
      "\n",
      "collaboration:(doc:0,tf:1), :(doc:2,tf:3), \n",
      "\n",
      "goal:(doc:0,tf:1), :(doc:4,tf:1), \n",
      "\n",
      "education:(doc:0,tf:5), :(doc:1,tf:2), :(doc:2,tf:2), :(doc:5,tf:1), \n",
      "\n",
      "train:(doc:0,tf:1), \n",
      "\n",
      "physician:(doc:0,tf:3), :(doc:1,tf:1), \n",
      "\n",
      "technologist:(doc:0,tf:1), \n",
      "\n",
      "locally:(doc:0,tf:1), \n",
      "\n",
      "worldwide:(doc:0,tf:1), \n",
      "\n",
      "latest:(doc:0,tf:1), \n",
      "\n",
      "development:(doc:0,tf:2), :(doc:2,tf:2), \n",
      "\n",
      "research:(doc:0,tf:6), :(doc:1,tf:1), :(doc:3,tf:3), \n",
      "\n",
      "new:(doc:0,tf:4), :(doc:1,tf:1), :(doc:5,tf:3), \n",
      "\n",
      "approach:(doc:0,tf:2), :(doc:3,tf:1), \n",
      "\n",
      "exploration:(doc:0,tf:1), \n",
      "\n",
      "assesment:(doc:0,tf:1), \n",
      "\n",
      "diagnostic:(doc:0,tf:2), \n",
      "\n",
      "image:(doc:0,tf:1), :(doc:5,tf:1), \n",
      "\n",
      "result:(doc:0,tf:1), :(doc:3,tf:1), \n",
      "\n",
      "cost:(doc:0,tf:1), :(doc:2,tf:1), \n",
      "\n",
      "effective:(doc:0,tf:1), :(doc:2,tf:2), \n",
      "\n",
      "b:(doc:0,tf:1), \n",
      "\n",
      "design:(doc:0,tf:1), :(doc:3,tf:7), \n",
      "\n",
      "planning:(doc:0,tf:1), :(doc:2,tf:1), \n",
      "\n",
      "monitoring:(doc:0,tf:1), :(doc:3,tf:1), \n",
      "\n",
      "therapy:(doc:0,tf:1), \n",
      "\n",
      "patient:(doc:0,tf:4), :(doc:4,tf:1), \n",
      "\n",
      "care:(doc:0,tf:5), :(doc:1,tf:2), \n",
      "\n",
      "deliver:(doc:0,tf:1), \n",
      "\n",
      "valid:(doc:0,tf:1), \n",
      "\n",
      "clinically:(doc:0,tf:1), \n",
      "\n",
      "relevant:(doc:0,tf:1), \n",
      "\n",
      "visualization:(doc:0,tf:1), \n",
      "\n",
      "community:(doc:0,tf:4), :(doc:1,tf:4), :(doc:2,tf:11), \n",
      "\n",
      "location:(doc:0,tf:1), :(doc:1,tf:1), \n",
      "\n",
      "richard:(doc:0,tf:1), \n",
      "\n",
      "lucas:(doc:0,tf:1), \n",
      "\n",
      "magnetic:(doc:0,tf:1), \n",
      "\n",
      "resonance:(doc:0,tf:1), \n",
      "\n",
      "center:(doc:0,tf:4), :(doc:1,tf:3), :(doc:2,tf:10), :(doc:4,tf:1), :(doc:5,tf:1), \n",
      "\n",
      "1201:(doc:0,tf:1), \n",
      "\n",
      "welch:(doc:0,tf:1), \n",
      "\n",
      "rd:(doc:0,tf:1), \n",
      "\n",
      "p170:(doc:0,tf:1), \n",
      "\n",
      "ca:(doc:0,tf:2), :(doc:5,tf:1), \n",
      "\n",
      "94305:(doc:0,tf:2), :(doc:5,tf:1), \n",
      "\n",
      "5488:(doc:0,tf:1), \n",
      "\n",
      "650:(doc:0,tf:2), :(doc:5,tf:1), \n",
      "\n",
      "725:(doc:0,tf:2), \n",
      "\n",
      "8432:(doc:0,tf:1), \n",
      "\n",
      "james:(doc:0,tf:1), \n",
      "\n",
      "h:(doc:0,tf:1), \n",
      "\n",
      "clark:(doc:0,tf:1), \n",
      "\n",
      "318:(doc:0,tf:1), \n",
      "\n",
      "campus:(doc:0,tf:1), :(doc:1,tf:4), \n",
      "\n",
      "drive:(doc:0,tf:1), \n",
      "\n",
      "s344:(doc:0,tf:1), \n",
      "\n",
      "5450:(doc:0,tf:1), \n",
      "\n",
      "6862:(doc:0,tf:1), \n",
      "\n",
      "direction:(doc:0,tf:3), :(doc:1,tf:1), \n",
      "\n",
      "3qd:(doc:0,tf:1), \n",
      "\n",
      "local:(doc:0,tf:1), \n",
      "\n",
      "hotel:(doc:0,tf:1), :(doc:1,tf:1), \n",
      "\n",
      "3dq:(doc:0,tf:4), \n",
      "\n",
      "laboratory:(doc:0,tf:2), :(doc:3,tf:1), \n",
      "\n",
      "navigation:(doc:0,tf:2), :(doc:1,tf:1), :(doc:3,tf:1), :(doc:4,tf:2), \n",
      "\n",
      "section:(doc:0,tf:1), :(doc:4,tf:1), \n",
      "\n",
      "home:(doc:0,tf:2), :(doc:4,tf:2), \n",
      "\n",
      "industry:(doc:0,tf:2), \n",
      "\n",
      "overview:(doc:0,tf:9), :(doc:1,tf:5), \n",
      "\n",
      "visiting:(doc:0,tf:1), \n",
      "\n",
      "fellowship:(doc:0,tf:1), \n",
      "\n",
      "reading:(doc:0,tf:1), \n",
      "\n",
      "list:(doc:0,tf:1), :(doc:5,tf:1), \n",
      "\n",
      "activity:(doc:0,tf:1), :(doc:1,tf:2), :(doc:2,tf:3), \n",
      "\n",
      "opportunity:(doc:0,tf:1), \n",
      "\n",
      "protocol:(doc:0,tf:1), \n",
      "\n",
      "management:(doc:0,tf:1), \n",
      "\n",
      "software:(doc:0,tf:1), :(doc:5,tf:4), \n",
      "\n",
      "case:(doc:0,tf:1), :(doc:3,tf:1), \n",
      "\n",
      "study:(doc:0,tf:1), :(doc:4,tf:3), \n",
      "\n",
      "testimonial:(doc:0,tf:1), \n",
      "\n",
      "infrastructure:(doc:0,tf:1), \n",
      "\n",
      "service:(doc:0,tf:2), :(doc:1,tf:1), :(doc:2,tf:3), :(doc:5,tf:6), \n",
      "\n",
      "faculty:(doc:0,tf:2), :(doc:1,tf:2), :(doc:3,tf:2), :(doc:5,tf:1), \n",
      "\n",
      "staff:(doc:0,tf:1), :(doc:2,tf:3), :(doc:5,tf:1), \n",
      "\n",
      "history:(doc:0,tf:1), \n",
      "\n",
      "resource:(doc:0,tf:3), :(doc:1,tf:2), :(doc:5,tf:1), \n",
      "\n",
      "equipment:(doc:0,tf:1), :(doc:5,tf:1), \n",
      "\n",
      "contact:(doc:0,tf:3), :(doc:1,tf:3), :(doc:3,tf:1), :(doc:4,tf:1), :(doc:5,tf:2), \n",
      "\n",
      "information:(doc:0,tf:1), :(doc:1,tf:1), :(doc:5,tf:1), \n",
      "\n",
      "job:(doc:0,tf:1), :(doc:5,tf:1), \n",
      "\n",
      "getting:(doc:0,tf:2), :(doc:1,tf:1), \n",
      "\n",
      "clinical:(doc:0,tf:2), :(doc:1,tf:3), \n",
      "\n",
      "hospital:(doc:0,tf:2), :(doc:1,tf:3), \n",
      "\n",
      "&:(doc:0,tf:7), :(doc:1,tf:7), :(doc:2,tf:2), :(doc:3,tf:2), :(doc:4,tf:1), :(doc:5,tf:1), \n",
      "\n",
      "clinic:(doc:0,tf:1), :(doc:1,tf:1), \n",
      "\n",
      "lucile:(doc:0,tf:1), :(doc:1,tf:1), \n",
      "\n",
      "packard:(doc:0,tf:1), :(doc:1,tf:1), \n",
      "\n",
      "child:(doc:0,tf:1), :(doc:1,tf:1), \n",
      "\n",
      "'s:(doc:0,tf:2), :(doc:1,tf:4), :(doc:2,tf:2), :(doc:4,tf:1), \n",
      "\n",
      "emergency:(doc:0,tf:1), :(doc:1,tf:1), \n",
      "\n",
      "news:(doc:0,tf:2), :(doc:1,tf:2), :(doc:5,tf:1), \n",
      "\n",
      "trial:(doc:0,tf:1), :(doc:1,tf:1), \n",
      "\n",
      "institute:(doc:0,tf:1), :(doc:1,tf:1), \n",
      "\n",
      "profile:(doc:0,tf:1), :(doc:1,tf:1), \n",
      "\n",
      "training:(doc:0,tf:1), :(doc:1,tf:1), :(doc:2,tf:1), :(doc:5,tf:1), \n",
      "\n",
      "program:(doc:0,tf:1), :(doc:1,tf:5), :(doc:2,tf:7), :(doc:3,tf:2), :(doc:4,tf:1), \n",
      "\n",
      "admission:(doc:0,tf:1), :(doc:1,tf:1), :(doc:3,tf:1), \n",
      "\n",
      "continuing:(doc:0,tf:1), :(doc:1,tf:1), \n",
      "\n",
      "health:(doc:0,tf:1), :(doc:1,tf:1), :(doc:3,tf:1), \n",
      "\n",
      "newsletter:(doc:0,tf:2), :(doc:1,tf:2), \n",
      "\n",
      "volunteering:(doc:0,tf:1), :(doc:1,tf:1), \n",
      "\n",
      "public:(doc:0,tf:1), :(doc:1,tf:1), :(doc:5,tf:2), \n",
      "\n",
      "partnership:(doc:0,tf:1), :(doc:1,tf:1), \n",
      "\n",
      "renewal:(doc:0,tf:1), :(doc:1,tf:1), \n",
      "\n",
      "building:(doc:0,tf:1), :(doc:1,tf:2), :(doc:2,tf:2), \n",
      "\n",
      "project:(doc:0,tf:1), :(doc:1,tf:1), :(doc:2,tf:5), :(doc:5,tf:2), \n",
      "\n",
      "career:(doc:0,tf:1), :(doc:1,tf:1), \n",
      "\n",
      "map:(doc:0,tf:1), :(doc:1,tf:1), \n",
      "\n",
      "dean:(doc:0,tf:1), :(doc:1,tf:1), \n",
      "\n",
      "footer:(doc:0,tf:1), :(doc:1,tf:1), :(doc:4,tf:1), :(doc:5,tf:1), \n",
      "\n",
      "link:(doc:0,tf:1), :(doc:1,tf:2), :(doc:2,tf:1), :(doc:3,tf:2), :(doc:4,tf:2), :(doc:5,tf:2), \n",
      "\n",
      "member:(doc:0,tf:1), :(doc:4,tf:2), \n",
      "\n",
      "2009:(doc:0,tf:1), :(doc:1,tf:1), :(doc:4,tf:1), \n",
      "\n",
      "term:(doc:0,tf:1), :(doc:1,tf:1), :(doc:4,tf:1), :(doc:5,tf:1), \n",
      "\n",
      "use:(doc:0,tf:1), :(doc:1,tf:1), :(doc:4,tf:1), :(doc:5,tf:1), \n",
      "\n",
      "powered:(doc:0,tf:1), :(doc:1,tf:1), :(doc:4,tf:1), \n",
      "\n",
      "irt:(doc:0,tf:1), :(doc:1,tf:1), :(doc:4,tf:1), \n",
      "\n",
      "anesthesia:(doc:1,tf:10), \n",
      "\n",
      "50th:(doc:1,tf:8), \n",
      "\n",
      "year:(doc:1,tf:6), :(doc:5,tf:1), \n",
      "\n",
      "celebration:(doc:1,tf:7), \n",
      "\n",
      "enhanced:(doc:1,tf:1), \n",
      "\n",
      "experience:(doc:1,tf:1), :(doc:2,tf:1), :(doc:5,tf:1), \n",
      "\n",
      "get:(doc:1,tf:1), \n",
      "\n",
      "free:(doc:1,tf:1), \n",
      "\n",
      "flash:(doc:1,tf:1), \n",
      "\n",
      "player:(doc:1,tf:1), \n",
      "\n",
      "thing:(doc:1,tf:1), \n",
      "\n",
      "weekend:(doc:1,tf:2), :(doc:2,tf:1), \n",
      "\n",
      "schedule:(doc:1,tf:1), \n",
      "\n",
      "scientific:(doc:1,tf:2), \n",
      "\n",
      "welcome:(doc:1,tf:1), \n",
      "\n",
      "dear:(doc:1,tf:1), \n",
      "\n",
      "approaching:(doc:1,tf:1), \n",
      "\n",
      "anniversary:(doc:1,tf:1), \n",
      "\n",
      "founding:(doc:1,tf:1), \n",
      "\n",
      "separate:(doc:1,tf:1), \n",
      "\n",
      "moved:(doc:1,tf:1), \n",
      "\n",
      "palo:(doc:1,tf:1), \n",
      "\n",
      "alto:(doc:1,tf:1), \n",
      "\n",
      "san:(doc:1,tf:1), \n",
      "\n",
      "francisco:(doc:1,tf:1), \n",
      "\n",
      "1959:(doc:1,tf:1), \n",
      "\n",
      "one:(doc:1,tf:2), :(doc:2,tf:3), \n",
      "\n",
      "later:(doc:1,tf:1), \n",
      "\n",
      "dr:(doc:1,tf:1), \n",
      "\n",
      "john:(doc:1,tf:1), \n",
      "\n",
      "bunker:(doc:1,tf:1), \n",
      "\n",
      "named:(doc:1,tf:1), \n",
      "\n",
      "first:(doc:1,tf:1), \n",
      "\n",
      "chair:(doc:1,tf:2), \n",
      "\n",
      "previously:(doc:1,tf:1), \n",
      "\n",
      "division:(doc:1,tf:1), :(doc:5,tf:1), \n",
      "\n",
      "within:(doc:1,tf:1), \n",
      "\n",
      "surgery:(doc:1,tf:1), \n",
      "\n",
      "celebrate:(doc:1,tf:2), \n",
      "\n",
      "50:(doc:1,tf:1), \n",
      "\n",
      "premier:(doc:1,tf:1), \n",
      "\n",
      "united:(doc:1,tf:1), \n",
      "\n",
      "state:(doc:1,tf:1), \n",
      "\n",
      "invite:(doc:1,tf:1), \n",
      "\n",
      "guest:(doc:1,tf:2), \n",
      "\n",
      "farm:(doc:1,tf:2), \n",
      "\n",
      "september:(doc:1,tf:1), \n",
      "\n",
      "24:(doc:1,tf:1), \n",
      "\n",
      "25:(doc:1,tf:1), :(doc:5,tf:1), \n",
      "\n",
      "2010:(doc:1,tf:1), \n",
      "\n",
      "kick:(doc:1,tf:1), \n",
      "\n",
      "friday:(doc:1,tf:1), :(doc:2,tf:1), \n",
      "\n",
      "evening:(doc:1,tf:3), \n",
      "\n",
      "reception:(doc:1,tf:1), \n",
      "\n",
      "rodin:(doc:1,tf:1), \n",
      "\n",
      "sculpture:(doc:1,tf:1), \n",
      "\n",
      "garden:(doc:1,tf:1), \n",
      "\n",
      "cantor:(doc:1,tf:1), \n",
      "\n",
      "museum:(doc:1,tf:1), \n",
      "\n",
      "saturday:(doc:1,tf:4), :(doc:2,tf:1), \n",
      "\n",
      "morning:(doc:1,tf:1), \n",
      "\n",
      "include:(doc:1,tf:1), :(doc:5,tf:1), \n",
      "\n",
      "session:(doc:1,tf:1), \n",
      "\n",
      "consisting:(doc:1,tf:1), \n",
      "\n",
      "discussion:(doc:1,tf:1), :(doc:2,tf:1), \n",
      "\n",
      "current:(doc:1,tf:3), \n",
      "\n",
      "former:(doc:1,tf:1), \n",
      "\n",
      "colleague:(doc:1,tf:2), \n",
      "\n",
      "provide:(doc:1,tf:1), \n",
      "\n",
      "update:(doc:1,tf:1), \n",
      "\n",
      "related:(doc:1,tf:1), \n",
      "\n",
      "practice:(doc:1,tf:1), \n",
      "\n",
      "reminiscence:(doc:1,tf:1), \n",
      "\n",
      "major:(doc:1,tf:1), \n",
      "\n",
      "accomplishment:(doc:1,tf:1), \n",
      "\n",
      "afternoon:(doc:1,tf:1), \n",
      "\n",
      "left:(doc:1,tf:1), \n",
      "\n",
      "open:(doc:1,tf:1), :(doc:2,tf:1), \n",
      "\n",
      "optional:(doc:1,tf:2), \n",
      "\n",
      "hike:(doc:1,tf:1), \n",
      "\n",
      "visit:(doc:1,tf:2), :(doc:5,tf:1), \n",
      "\n",
      "pacific:(doc:1,tf:1), \n",
      "\n",
      "ocean:(doc:1,tf:1), \n",
      "\n",
      "tour:(doc:1,tf:1), \n",
      "\n",
      "etc:(doc:1,tf:1), \n",
      "\n",
      "time:(doc:1,tf:2), :(doc:3,tf:2), \n",
      "\n",
      "enjoy:(doc:1,tf:1), \n",
      "\n",
      "special:(doc:1,tf:1), \n",
      "\n",
      "place:(doc:1,tf:1), \n",
      "\n",
      "people:(doc:1,tf:1), \n",
      "\n",
      "bay:(doc:1,tf:1), \n",
      "\n",
      "area:(doc:1,tf:1), \n",
      "\n",
      "feature:(doc:1,tf:1), \n",
      "\n",
      "black:(doc:1,tf:1), :(doc:2,tf:2), \n",
      "\n",
      "tie:(doc:1,tf:1), \n",
      "\n",
      "dinner:(doc:1,tf:1), \n",
      "\n",
      "francis:(doc:1,tf:1), \n",
      "\n",
      "c:(doc:1,tf:1), \n",
      "\n",
      "arrillaga:(doc:1,tf:1), \n",
      "\n",
      "sincerely:(doc:1,tf:2), \n",
      "\n",
      "hope:(doc:1,tf:1), \n",
      "\n",
      "able:(doc:1,tf:1), \n",
      "\n",
      "return:(doc:1,tf:1), \n",
      "\n",
      "friend:(doc:1,tf:1), \n",
      "\n",
      "attendance:(doc:1,tf:1), \n",
      "\n",
      "limited:(doc:1,tf:1), :(doc:2,tf:1), \n",
      "\n",
      "450:(doc:1,tf:1), \n",
      "\n",
      "participant:(doc:1,tf:1), :(doc:2,tf:5), \n",
      "\n",
      "including:(doc:1,tf:1), :(doc:5,tf:1), \n",
      "\n",
      "spouse:(doc:1,tf:1), \n",
      "\n",
      "due:(doc:1,tf:1), \n",
      "\n",
      "constraint:(doc:1,tf:1), \n",
      "\n",
      "please:(doc:1,tf:2), :(doc:2,tf:1), :(doc:5,tf:1), \n",
      "\n",
      "mark:(doc:1,tf:1), \n",
      "\n",
      "calendar:(doc:1,tf:1), :(doc:5,tf:1), \n",
      "\n",
      "register:(doc:1,tf:2), \n",
      "\n",
      "early:(doc:1,tf:1), \n",
      "\n",
      "need:(doc:1,tf:1), :(doc:5,tf:1), \n",
      "\n",
      "website:(doc:1,tf:2), :(doc:5,tf:2), \n",
      "\n",
      "question:(doc:1,tf:1), :(doc:5,tf:2), \n",
      "\n",
      "comment:(doc:1,tf:1), :(doc:5,tf:2), \n",
      "\n",
      "reached:(doc:1,tf:1), \n",
      "\n",
      "mhr:(doc:1,tf:1), \n",
      "\n",
      "edu:(doc:1,tf:1), \n",
      "\n",
      "myer:(doc:1,tf:1), \n",
      "\n",
      "mike:(doc:1,tf:1), \n",
      "\n",
      "rosenthal:(doc:1,tf:1), \n",
      "\n",
      "md:(doc:1,tf:1), \n",
      "\n",
      "committee:(doc:1,tf:1), \n",
      "\n",
      "registration:(doc:1,tf:2), \n",
      "\n",
      "fax:(doc:1,tf:1), \n",
      "\n",
      "mail:(doc:1,tf:1), \n",
      "\n",
      "consider:(doc:1,tf:1), \n",
      "\n",
      "making:(doc:1,tf:1), \n",
      "\n",
      "donation:(doc:1,tf:1), \n",
      "\n",
      "made:(doc:1,tf:1), \n",
      "\n",
      "aim:(doc:1,tf:1), \n",
      "\n",
      "nav:(doc:1,tf:24), \n",
      "\n",
      "1:(doc:1,tf:1), \n",
      "\n",
      "2:(doc:1,tf:1), \n",
      "\n",
      "2_1:(doc:1,tf:1), \n",
      "\n",
      "2_2:(doc:1,tf:1), \n",
      "\n",
      "2_3:(doc:1,tf:1), \n",
      "\n",
      "3:(doc:1,tf:1), \n",
      "\n",
      "3_1:(doc:1,tf:1), \n",
      "\n",
      "3_2:(doc:1,tf:1), \n",
      "\n",
      "3_3:(doc:1,tf:1), \n",
      "\n",
      "3_3_1:(doc:1,tf:1), \n",
      "\n",
      "3_3_2:(doc:1,tf:1), \n",
      "\n",
      "3_3_3:(doc:1,tf:1), \n",
      "\n",
      "4:(doc:1,tf:1), :(doc:2,tf:2), \n",
      "\n",
      "4_1:(doc:1,tf:1), \n",
      "\n",
      "4_2:(doc:1,tf:1), \n",
      "\n",
      "4_3:(doc:1,tf:1), \n",
      "\n",
      "5:(doc:1,tf:1), \n",
      "\n",
      "5_1:(doc:1,tf:1), \n",
      "\n",
      "5_2:(doc:1,tf:1), \n",
      "\n",
      "5_3:(doc:1,tf:1), \n",
      "\n",
      "6:(doc:1,tf:1), :(doc:2,tf:1), \n",
      "\n",
      "6_1:(doc:1,tf:1), \n",
      "\n",
      "6_2:(doc:1,tf:1), \n",
      "\n",
      "6_3:(doc:1,tf:1), \n",
      "\n",
      "a3c:(doc:2,tf:1), \n",
      "\n",
      "lead:(doc:2,tf:10), \n",
      "\n",
      "leading:(doc:2,tf:2), \n",
      "\n",
      "activism:(doc:2,tf:2), \n",
      "\n",
      "diversity:(doc:2,tf:2), \n",
      "\n",
      "fill:(doc:2,tf:2), \n",
      "\n",
      "application:(doc:2,tf:4), :(doc:3,tf:1), \n",
      "\n",
      "read:(doc:2,tf:2), :(doc:5,tf:4), \n",
      "\n",
      "text:(doc:2,tf:2), \n",
      "\n",
      "carefully:(doc:2,tf:1), \n",
      "\n",
      "color:(doc:2,tf:2), \n",
      "\n",
      "asian:(doc:2,tf:2), \n",
      "\n",
      "american:(doc:2,tf:2), \n",
      "\n",
      "el:(doc:2,tf:2), \n",
      "\n",
      "centro:(doc:2,tf:2), \n",
      "\n",
      "chicano:(doc:2,tf:2), \n",
      "\n",
      "proud:(doc:2,tf:1), \n",
      "\n",
      "announce:(doc:2,tf:1), \n",
      "\n",
      "leadership:(doc:2,tf:7), \n",
      "\n",
      "course:(doc:2,tf:2), \n",
      "\n",
      "undergraduate:(doc:2,tf:1), \n",
      "\n",
      "student:(doc:2,tf:11), :(doc:3,tf:1), :(doc:5,tf:3), \n",
      "\n",
      "interested:(doc:2,tf:1), \n",
      "\n",
      "exploring:(doc:2,tf:1), \n",
      "\n",
      "issue:(doc:2,tf:1), \n",
      "\n",
      "across:(doc:2,tf:2), \n",
      "\n",
      "ethnic:(doc:2,tf:2), \n",
      "\n",
      "consists:(doc:2,tf:1), \n",
      "\n",
      "fall:(doc:2,tf:2), \n",
      "\n",
      "quarter:(doc:2,tf:3), :(doc:5,tf:1), \n",
      "\n",
      "retreat:(doc:2,tf:8), \n",
      "\n",
      "fellow:(doc:2,tf:1), \n",
      "\n",
      "continues:(doc:2,tf:1), \n",
      "\n",
      "weekly:(doc:2,tf:3), \n",
      "\n",
      "class:(doc:2,tf:2), :(doc:5,tf:3), \n",
      "\n",
      "meeting:(doc:2,tf:4), \n",
      "\n",
      "culminates:(doc:2,tf:1), \n",
      "\n",
      "designed:(doc:2,tf:2), \n",
      "\n",
      "executed:(doc:2,tf:1), \n",
      "\n",
      "social:(doc:2,tf:12), \n",
      "\n",
      "change:(doc:2,tf:12), \n",
      "\n",
      "winter:(doc:2,tf:2), \n",
      "\n",
      "may:(doc:2,tf:1), \n",
      "\n",
      "receive:(doc:2,tf:1), \n",
      "\n",
      "unit:(doc:2,tf:1), \n",
      "\n",
      "academic:(doc:2,tf:1), :(doc:5,tf:7), \n",
      "\n",
      "credit:(doc:2,tf:1), \n",
      "\n",
      "per:(doc:2,tf:2), \n",
      "\n",
      "participation:(doc:2,tf:1), \n",
      "\n",
      "model:(doc:2,tf:8), :(doc:3,tf:2), \n",
      "\n",
      "professional:(doc:2,tf:1), \n",
      "\n",
      "offered:(doc:2,tf:1), \n",
      "\n",
      "since:(doc:2,tf:2), \n",
      "\n",
      "1999:(doc:2,tf:1), \n",
      "\n",
      "emerging:(doc:2,tf:1), \n",
      "\n",
      "leader:(doc:2,tf:1), \n",
      "\n",
      "based:(doc:2,tf:2), :(doc:3,tf:3), \n",
      "\n",
      "believe:(doc:2,tf:1), \n",
      "\n",
      "framework:(doc:2,tf:1), :(doc:3,tf:1), \n",
      "\n",
      "collaborative:(doc:2,tf:2), \n",
      "\n",
      "non:(doc:2,tf:2), \n",
      "\n",
      "hierarchical:(doc:2,tf:2), \n",
      "\n",
      "useful:(doc:2,tf:1), \n",
      "\n",
      "tool:(doc:2,tf:1), \n",
      "\n",
      "searching:(doc:2,tf:1), \n",
      "\n",
      "method:(doc:2,tf:1), :(doc:3,tf:2), \n",
      "\n",
      "working:(doc:2,tf:1), \n",
      "\n",
      "emphasizes:(doc:2,tf:1), \n",
      "\n",
      "value:(doc:2,tf:3), \n",
      "\n",
      "mean:(doc:2,tf:1), \n",
      "\n",
      "diverse:(doc:2,tf:2), \n",
      "\n",
      "group:(doc:2,tf:3), :(doc:4,tf:1), \n",
      "\n",
      "individual:(doc:2,tf:1), \n",
      "\n",
      "create:(doc:2,tf:1), \n",
      "\n",
      "movement:(doc:2,tf:1), \n",
      "\n",
      "instill:(doc:2,tf:1), \n",
      "\n",
      "young:(doc:2,tf:1), \n",
      "\n",
      "strong:(doc:2,tf:1), :(doc:3,tf:1), \n",
      "\n",
      "sense:(doc:2,tf:1), \n",
      "\n",
      "civic:(doc:2,tf:1), \n",
      "\n",
      "responsibility:(doc:2,tf:1), \n",
      "\n",
      "desire:(doc:2,tf:2), \n",
      "\n",
      "seven:(doc:2,tf:1), \n",
      "\n",
      "core:(doc:2,tf:2), \n",
      "\n",
      "consciousness:(doc:2,tf:1), \n",
      "\n",
      "self:(doc:2,tf:1), \n",
      "\n",
      "congruence:(doc:2,tf:1), \n",
      "\n",
      "commitment:(doc:2,tf:1), \n",
      "\n",
      "common:(doc:2,tf:1), :(doc:4,tf:1), \n",
      "\n",
      "purpose:(doc:2,tf:2), \n",
      "\n",
      "controversy:(doc:2,tf:1), \n",
      "\n",
      "civility:(doc:2,tf:1), \n",
      "\n",
      "citizenship:(doc:2,tf:1), \n",
      "\n",
      "stressed:(doc:2,tf:1), \n",
      "\n",
      "reinforced:(doc:2,tf:1), \n",
      "\n",
      "action:(doc:2,tf:1), \n",
      "\n",
      "creation:(doc:2,tf:1), \n",
      "\n",
      "also:(doc:2,tf:2), \n",
      "\n",
      "included:(doc:2,tf:1), \n",
      "\n",
      "eighth:(doc:2,tf:1), \n",
      "\n",
      "orient:(doc:2,tf:1), \n",
      "\n",
      "experiential:(doc:2,tf:1), \n",
      "\n",
      "exercise:(doc:2,tf:1), \n",
      "\n",
      "following:(doc:2,tf:2), \n",
      "\n",
      "continue:(doc:2,tf:1), \n",
      "\n",
      "learn:(doc:2,tf:1), :(doc:5,tf:1), \n",
      "\n",
      "plan:(doc:2,tf:1), \n",
      "\n",
      "execute:(doc:2,tf:1), \n",
      "\n",
      "small:(doc:2,tf:2), \n",
      "\n",
      "setting:(doc:2,tf:1), \n",
      "\n",
      "thereby:(doc:2,tf:1), \n",
      "\n",
      "increasing:(doc:2,tf:1), \n",
      "\n",
      "understanding:(doc:2,tf:1), \n",
      "\n",
      "practical:(doc:2,tf:1), \n",
      "\n",
      "selection:(doc:2,tf:1), \n",
      "\n",
      "process:(doc:2,tf:1), \n",
      "\n",
      "space:(doc:2,tf:1), :(doc:3,tf:1), :(doc:5,tf:1), \n",
      "\n",
      "8:(doc:2,tf:1), \n",
      "\n",
      "selected:(doc:2,tf:1), \n",
      "\n",
      "consideration:(doc:2,tf:1), \n",
      "\n",
      "given:(doc:2,tf:1), \n",
      "\n",
      "representative:(doc:2,tf:1), \n",
      "\n",
      "sample:(doc:2,tf:1), \n",
      "\n",
      "prior:(doc:2,tf:2), \n",
      "\n",
      "involvement:(doc:2,tf:1), \n",
      "\n",
      "required:(doc:2,tf:1), \n",
      "\n",
      "logistics:(doc:2,tf:1), \n",
      "\n",
      "transportation:(doc:2,tf:1), \n",
      "\n",
      "overnight:(doc:2,tf:1), \n",
      "\n",
      "accommodation:(doc:2,tf:1), \n",
      "\n",
      "meal:(doc:2,tf:1), \n",
      "\n",
      "provided:(doc:2,tf:1), \n",
      "\n",
      "funded:(doc:2,tf:1), \n",
      "\n",
      "expectation:(doc:2,tf:1), \n",
      "\n",
      "main:(doc:2,tf:1), \n",
      "\n",
      "foster:(doc:2,tf:1), \n",
      "\n",
      "cross:(doc:2,tf:1), \n",
      "\n",
      "requires:(doc:2,tf:1), \n",
      "\n",
      "present:(doc:2,tf:2), \n",
      "\n",
      "requirement:(doc:2,tf:1), \n",
      "\n",
      "mandatory:(doc:2,tf:1), \n",
      "\n",
      "attend:(doc:2,tf:2), \n",
      "\n",
      "pre:(doc:2,tf:1), \n",
      "\n",
      "thursday:(doc:2,tf:2), \n",
      "\n",
      "entire:(doc:2,tf:1), \n",
      "\n",
      "offsite:(doc:2,tf:1), \n",
      "\n",
      "october:(doc:2,tf:2), \n",
      "\n",
      "24th:(doc:2,tf:1), \n",
      "\n",
      "25th:(doc:2,tf:1), \n",
      "\n",
      "every:(doc:2,tf:1), \n",
      "\n",
      "30:(doc:2,tf:1), \n",
      "\n",
      "00pm:(doc:2,tf:1), \n",
      "\n",
      "complete:(doc:2,tf:1), \n",
      "\n",
      "determined:(doc:2,tf:1), \n",
      "\n",
      "assigned:(doc:2,tf:1), \n",
      "\n",
      "wish:(doc:2,tf:1), \n",
      "\n",
      "click:(doc:2,tf:1), \n",
      "\n",
      "would:(doc:2,tf:1), \n",
      "\n",
      "like:(doc:2,tf:1), \n",
      "\n",
      "aeronautics:(doc:3,tf:2), \n",
      "\n",
      "astronautics:(doc:3,tf:2), \n",
      "\n",
      "aeroastro:(doc:3,tf:2), \n",
      "\n",
      "logo:(doc:3,tf:1), :(doc:5,tf:1), \n",
      "\n",
      "computational:(doc:3,tf:7), \n",
      "\n",
      "last:(doc:3,tf:1), \n",
      "\n",
      "two:(doc:3,tf:1), \n",
      "\n",
      "decade:(doc:3,tf:1), \n",
      "\n",
      "giant:(doc:3,tf:1), \n",
      "\n",
      "stride:(doc:3,tf:1), \n",
      "\n",
      "achieved:(doc:3,tf:1), \n",
      "\n",
      "many:(doc:3,tf:1), :(doc:4,tf:1), \n",
      "\n",
      "aspect:(doc:3,tf:1), \n",
      "\n",
      "aerospace:(doc:3,tf:4), \n",
      "\n",
      "engineering:(doc:3,tf:3), \n",
      "\n",
      "higher:(doc:3,tf:1), \n",
      "\n",
      "fidelity:(doc:3,tf:2), \n",
      "\n",
      "mathematical:(doc:3,tf:1), \n",
      "\n",
      "better:(doc:3,tf:1), \n",
      "\n",
      "approximation:(doc:3,tf:1), \n",
      "\n",
      "faster:(doc:3,tf:1), \n",
      "\n",
      "solution:(doc:3,tf:1), \n",
      "\n",
      "algorithm:(doc:3,tf:2), \n",
      "\n",
      "developed:(doc:3,tf:1), \n",
      "\n",
      "aerodynamic:(doc:3,tf:1), \n",
      "\n",
      "structural:(doc:3,tf:2), \n",
      "\n",
      "aeroacoustic:(doc:3,tf:1), \n",
      "\n",
      "aeroelastic:(doc:3,tf:1), \n",
      "\n",
      "aerothermal:(doc:3,tf:1), \n",
      "\n",
      "control:(doc:3,tf:2), \n",
      "\n",
      "among:(doc:3,tf:1), \n",
      "\n",
      "others:(doc:3,tf:1), \n",
      "\n",
      "computing:(doc:3,tf:3), :(doc:5,tf:6), \n",
      "\n",
      "speed:(doc:3,tf:1), \n",
      "\n",
      "barrier:(doc:3,tf:1), \n",
      "\n",
      "shattered:(doc:3,tf:1), \n",
      "\n",
      "hardware:(doc:3,tf:1), \n",
      "\n",
      "manufacturer:(doc:3,tf:1), \n",
      "\n",
      "parallel:(doc:3,tf:1), \n",
      "\n",
      "cluster:(doc:3,tf:1), \n",
      "\n",
      "become:(doc:3,tf:1), \n",
      "\n",
      "ubiquitous:(doc:3,tf:1), \n",
      "\n",
      "numerical:(doc:3,tf:1), \n",
      "\n",
      "simulation:(doc:3,tf:2), \n",
      "\n",
      "increasingly:(doc:3,tf:1), \n",
      "\n",
      "complemented:(doc:3,tf:1), \n",
      "\n",
      "replaced:(doc:3,tf:1), \n",
      "\n",
      "physical:(doc:3,tf:1), \n",
      "\n",
      "test:(doc:3,tf:1), \n",
      "\n",
      "enhance:(doc:3,tf:2), \n",
      "\n",
      "reliability:(doc:3,tf:1), \n",
      "\n",
      "improve:(doc:3,tf:1), \n",
      "\n",
      "productivity:(doc:3,tf:1), \n",
      "\n",
      "engineer:(doc:3,tf:1), \n",
      "\n",
      "reduce:(doc:3,tf:1), \n",
      "\n",
      "cycle:(doc:3,tf:1), \n",
      "\n",
      "system:(doc:3,tf:3), \n",
      "\n",
      "performance:(doc:3,tf:1), \n",
      "\n",
      "presence:(doc:3,tf:1), \n",
      "\n",
      "carried:(doc:3,tf:1), \n",
      "\n",
      "primarily:(doc:3,tf:1), \n",
      "\n",
      "frg:(doc:3,tf:1), \n",
      "\n",
      "focus:(doc:3,tf:1), \n",
      "\n",
      "multidisciplinary:(doc:3,tf:1), \n",
      "\n",
      "different:(doc:3,tf:1), \n",
      "\n",
      "physic:(doc:3,tf:1), \n",
      "\n",
      "pertaining:(doc:3,tf:1), \n",
      "\n",
      "multiscale:(doc:3,tf:1), \n",
      "\n",
      "deal:(doc:3,tf:1), \n",
      "\n",
      "large:(doc:3,tf:1), \n",
      "\n",
      "range:(doc:3,tf:1), \n",
      "\n",
      "spatial:(doc:3,tf:1), \n",
      "\n",
      "scale:(doc:3,tf:1), \n",
      "\n",
      "high:(doc:3,tf:1), \n",
      "\n",
      "scheme:(doc:3,tf:1), \n",
      "\n",
      "enable:(doc:3,tf:1), \n",
      "\n",
      "predictive:(doc:3,tf:1), \n",
      "\n",
      "optimization:(doc:3,tf:1), \n",
      "\n",
      "handle:(doc:3,tf:1), \n",
      "\n",
      "complex:(doc:3,tf:1), \n",
      "\n",
      "integrated:(doc:3,tf:1), \n",
      "\n",
      "reduction:(doc:3,tf:1), \n",
      "\n",
      "integrate:(doc:3,tf:1), \n",
      "\n",
      "computation:(doc:3,tf:1), \n",
      "\n",
      "1997:(doc:3,tf:1), \n",
      "\n",
      "2012:(doc:3,tf:1), :(doc:5,tf:1), \n",
      "\n",
      "right:(doc:3,tf:1), \n",
      "\n",
      "reserved:(doc:3,tf:1), \n",
      "\n",
      "sustainable:(doc:3,tf:1), \n",
      "\n",
      "aviation:(doc:3,tf:1), \n",
      "\n",
      "curriculum:(doc:3,tf:1), \n",
      "\n",
      "financial:(doc:3,tf:1), \n",
      "\n",
      "aid:(doc:3,tf:1), \n",
      "\n",
      "event:(doc:3,tf:1), \n",
      "\n",
      "affiliate:(doc:3,tf:1), \n",
      "\n",
      "membership:(doc:3,tf:1), \n",
      "\n",
      "giving:(doc:3,tf:1), \n",
      "\n",
      "aa:(doc:3,tf:1), \n",
      "\n",
      "al:(doc:4,tf:4), \n",
      "\n",
      "consortium:(doc:4,tf:4), \n",
      "\n",
      "epidemiological:(doc:4,tf:1), \n",
      "\n",
      "ace:(doc:4,tf:7), \n",
      "\n",
      "epidemiologic:(doc:4,tf:2), \n",
      "\n",
      "international:(doc:4,tf:1), \n",
      "\n",
      "researcher:(doc:4,tf:2), \n",
      "\n",
      "specialty:(doc:4,tf:1), \n",
      "\n",
      "whose:(doc:4,tf:1), \n",
      "\n",
      "identify:(doc:4,tf:1), \n",
      "\n",
      "environmental:(doc:4,tf:1), \n",
      "\n",
      "lifestyle:(doc:4,tf:1), \n",
      "\n",
      "genetic:(doc:4,tf:1), \n",
      "\n",
      "risk:(doc:4,tf:1), \n",
      "\n",
      "factor:(doc:4,tf:1), \n",
      "\n",
      "amyotrophic:(doc:4,tf:1), \n",
      "\n",
      "lateral:(doc:4,tf:1), \n",
      "\n",
      "sclerosis:(doc:4,tf:1), \n",
      "\n",
      "lou:(doc:4,tf:1), \n",
      "\n",
      "gehrig:(doc:4,tf:1), \n",
      "\n",
      "disease:(doc:4,tf:1), \n",
      "\n",
      "director:(doc:4,tf:2), \n",
      "\n",
      "lorene:(doc:4,tf:1), \n",
      "\n",
      "nelson:(doc:4,tf:1), \n",
      "\n",
      "phd:(doc:4,tf:2), \n",
      "\n",
      "m:(doc:4,tf:1), \n",
      "\n",
      "co:(doc:4,tf:1), \n",
      "\n",
      "valerie:(doc:4,tf:1), \n",
      "\n",
      "mcguire:(doc:4,tf:1), \n",
      "\n",
      "mph:(doc:4,tf:2), \n",
      "\n",
      "database:(doc:4,tf:1), \n",
      "\n",
      "manager:(doc:4,tf:1), \n",
      "\n",
      "barbara:(doc:4,tf:1), \n",
      "\n",
      "topol:(doc:4,tf:1), \n",
      "\n",
      "web:(doc:4,tf:1), \n",
      "\n",
      "page:(doc:4,tf:1), :(doc:5,tf:1), \n",
      "\n",
      "coordinator:(doc:4,tf:1), \n",
      "\n",
      "jacqueline:(doc:4,tf:1), \n",
      "\n",
      "itnyre:(doc:4,tf:1), \n",
      "\n",
      "ml:(doc:4,tf:1), \n",
      "\n",
      "administrative:(doc:4,tf:1), \n",
      "\n",
      "associate:(doc:4,tf:1), \n",
      "\n",
      "christine:(doc:4,tf:1), \n",
      "\n",
      "dorosin:(doc:4,tf:1), \n",
      "\n",
      "family:(doc:4,tf:1), \n",
      "\n",
      "additinal:(doc:4,tf:1), \n",
      "\n",
      "login:(doc:4,tf:2), :(doc:5,tf:2), \n",
      "\n",
      "ac:(doc:5,tf:3), \n",
      "\n",
      "block:(doc:5,tf:2), \n",
      "\n",
      "visitor:(doc:5,tf:1), \n",
      "\n",
      "help:(doc:5,tf:4), \n",
      "\n",
      "network:(doc:5,tf:2), \n",
      "\n",
      "connection:(doc:5,tf:2), \n",
      "\n",
      "computer:(doc:5,tf:3), \n",
      "\n",
      "device:(doc:5,tf:1), \n",
      "\n",
      "internet:(doc:5,tf:1), \n",
      "\n",
      "access:(doc:5,tf:2), \n",
      "\n",
      "multimedia:(doc:5,tf:4), \n",
      "\n",
      "studio:(doc:5,tf:1), \n",
      "\n",
      "audio:(doc:5,tf:1), \n",
      "\n",
      "video:(doc:5,tf:1), \n",
      "\n",
      "editing:(doc:5,tf:1), \n",
      "\n",
      "classroom:(doc:5,tf:1), \n",
      "\n",
      "support:(doc:5,tf:2), \n",
      "\n",
      "scheduling:(doc:5,tf:1), \n",
      "\n",
      "technology:(doc:5,tf:3), \n",
      "\n",
      "printing:(doc:5,tf:2), \n",
      "\n",
      "print:(doc:5,tf:1), \n",
      "\n",
      "machine:(doc:5,tf:1), \n",
      "\n",
      "central:(doc:5,tf:1), \n",
      "\n",
      "email:(doc:5,tf:1), \n",
      "\n",
      "afs:(doc:5,tf:1), \n",
      "\n",
      "tech:(doc:5,tf:3), \n",
      "\n",
      "helpsu:(doc:5,tf:1), \n",
      "\n",
      "blog:(doc:5,tf:1), \n",
      "\n",
      "checkout:(doc:5,tf:4), \n",
      "\n",
      "item:(doc:5,tf:2), \n",
      "\n",
      "sept:(doc:5,tf:1), \n",
      "\n",
      "2011:(doc:5,tf:2), \n",
      "\n",
      "updated:(doc:5,tf:1), \n",
      "\n",
      "inventory:(doc:5,tf:1), \n",
      "\n",
      "exciting:(doc:5,tf:1), \n",
      "\n",
      "meyer:(doc:5,tf:4), \n",
      "\n",
      "desk:(doc:5,tf:1), \n",
      "\n",
      "annual:(doc:5,tf:1), \n",
      "\n",
      "survey:(doc:5,tf:1), \n",
      "\n",
      "ask:(doc:5,tf:1), \n",
      "\n",
      "grad:(doc:5,tf:1), \n",
      "\n",
      "undergrad:(doc:5,tf:1), \n",
      "\n",
      "assu:(doc:5,tf:1), \n",
      "\n",
      "projector:(doc:5,tf:2), \n",
      "\n",
      "joseph:(doc:5,tf:1), \n",
      "\n",
      "kautz:(doc:5,tf:1), \n",
      "\n",
      "elected:(doc:5,tf:1), \n",
      "\n",
      "president:(doc:5,tf:1), \n",
      "\n",
      "swallt:(doc:5,tf:1), \n",
      "\n",
      "window:(doc:5,tf:2), \n",
      "\n",
      "7:(doc:5,tf:2), \n",
      "\n",
      "mac:(doc:5,tf:2), \n",
      "\n",
      "osx:(doc:5,tf:2), \n",
      "\n",
      "snow:(doc:5,tf:2), \n",
      "\n",
      "leopard:(doc:5,tf:2), \n",
      "\n",
      "managed:(doc:5,tf:1), \n",
      "\n",
      "upgraded:(doc:5,tf:1), \n",
      "\n",
      "detail:(doc:5,tf:1), \n",
      "\n",
      "installed:(doc:5,tf:1), \n",
      "\n",
      "montage:(doc:5,tf:1), \n",
      "\n",
      "cs2c:(doc:5,tf:1), \n",
      "\n",
      "production:(doc:5,tf:3), \n",
      "\n",
      "flex:(doc:5,tf:2), \n",
      "\n",
      "look:(doc:5,tf:1), \n",
      "\n",
      "graphic:(doc:5,tf:2), \n",
      "\n",
      "novel:(doc:5,tf:2), \n",
      "\n",
      "digital:(doc:5,tf:1), \n",
      "\n",
      "workflow:(doc:5,tf:1), \n",
      "\n",
      "used:(doc:5,tf:1), \n",
      "\n",
      "let:(doc:5,tf:2), \n",
      "\n",
      "know:(doc:5,tf:2), \n",
      "\n",
      "think:(doc:5,tf:2), \n",
      "\n",
      "trouble:(doc:5,tf:2), \n",
      "\n",
      "finding:(doc:5,tf:2), \n",
      "\n",
      "something:(doc:5,tf:2), \n",
      "\n",
      "submit:(doc:5,tf:2), \n",
      "\n",
      "2nd:(doc:5,tf:1), \n",
      "\n",
      "floor:(doc:5,tf:1), \n",
      "\n",
      "hour:(doc:5,tf:1), \n",
      "\n",
      "mon:(doc:5,tf:1), \n",
      "\n",
      "thu:(doc:5,tf:1), \n",
      "\n",
      "9am:(doc:5,tf:2), \n",
      "\n",
      "12:(doc:5,tf:2), \n",
      "\n",
      "midnight:(doc:5,tf:2), \n",
      "\n",
      "fri:(doc:5,tf:1), \n",
      "\n",
      "5pm:(doc:5,tf:2), \n",
      "\n",
      "sat:(doc:5,tf:1), \n",
      "\n",
      "1pm:(doc:5,tf:2), \n",
      "\n",
      "sun:(doc:5,tf:1), \n",
      "\n",
      "quick:(doc:5,tf:1), \n",
      "\n",
      "available:(doc:5,tf:1), \n",
      "\n",
      "coursework:(doc:5,tf:1), \n",
      "\n",
      "learning:(doc:5,tf:1), \n",
      "\n",
      "info:(doc:5,tf:2), \n",
      "\n",
      "ic:(doc:5,tf:1), \n",
      "\n",
      "residential:(doc:5,tf:1), \n",
      "\n",
      "user:(doc:5,tf:1), \n",
      "\n",
      "sunetid:(doc:5,tf:1), \n",
      "\n",
      "sulair:(doc:5,tf:1), \n",
      "\n",
      "723:(doc:5,tf:1), \n",
      "\n",
      "2300:(doc:5,tf:1), \n",
      "\n",
      "copyright:(doc:5,tf:1), \n",
      "\n",
      "complaint:(doc:5,tf:1), \n",
      "\n"
     ]
    }
   ],
   "source": [
    "for item in indexer.inverted_index.items():\n",
    "    term = item[0]\n",
    "    text = str(term)\n",
    "    for post in item[1].posts:\n",
    "        doc_id = post[0]\n",
    "        tf = post[1].tf\n",
    "        text = text + \":\" + \"(doc:\" + str(doc_id) + \",tf:\" + str(tf) + \"), \"\n",
    "    text = text + \"\\n\"\n",
    "    print(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Search query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def searching(base_dir, indexer, query_processor, query):\n",
    "    '''\n",
    "    Search given string query in index\n",
    "    :param base_dir:\n",
    "    :param indexer: indexer obj\n",
    "    :param query_processor: query processor obj\n",
    "    :param query: string query\n",
    "    :return:\n",
    "    '''\n",
    "    # print(query)\n",
    "    docs = getfilenames(base_dir=base_dir)\n",
    "    docs_length = len(docs)\n",
    "\n",
    "    op = np.array([False] * docs_length)\n",
    "    query_terms = query_processor.process(query)\n",
    "    results = indexer.search(query_terms)\n",
    "    for result in results:\n",
    "        docID, score = result\n",
    "        op[docID] = True\n",
    "        # docName = docs[docID]\n",
    "        # print(\"DocID: %d DocName: %s Score: %0.2f\" % (docID, docName, score))\n",
    "    return op"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Please enter query:medicine\n",
      "DocName: Doc1.txt\n",
      "DocName: Doc2.txt\n",
      "DocName: Doc5.txt\n",
      "Please enter query:NOT medicine\n",
      "DocName: Doc3.txt\n",
      "DocName: Doc4.txt\n",
      "DocName: Doc6.txt\n",
      "Please enter query:(Lab OR technology) AND (aeroastro OR Josef)\n",
      "DocName: Doc4.txt\n",
      "Please enter query:(medicine OR technology) AND (block OR design)\n",
      "DocName: Doc1.txt\n",
      "DocName: Doc6.txt\n",
      "Please enter query:q:\n"
     ]
    }
   ],
   "source": [
    "while (True):\n",
    "    query = input(\"Please enter query:\")\n",
    "    if query == \"q:\":\n",
    "        break\n",
    "    query = query.replace(\"(\", \" ( \")\n",
    "    query = query.replace(\")\", \" ) \")\n",
    "    str_eval = \"\"\n",
    "    for split_word in query.split():\n",
    "        if split_word == \"(\" or split_word == \")\":\n",
    "            str_eval += split_word + \" \"\n",
    "        elif split_word == \"OR\":\n",
    "            str_eval += \"| \"\n",
    "        elif split_word == \"AND\":\n",
    "            str_eval += \"& \"\n",
    "        elif split_word == \"NOT\":\n",
    "            str_eval += \"~ \"\n",
    "        else:\n",
    "            str_eval += \"searching(base_dir,indexer,query_processor, '\" + split_word + \"')\" + \" \"\n",
    "    # print(str_eval)\n",
    "\n",
    "    op = eval(str_eval)\n",
    "    for docID in np.where(np.array(op) == True)[0]:\n",
    "        docName = docs[docID]\n",
    "        print(\"DocName: %s\" % docName)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
